package scanner

import (
	"bufio"
	"context"
	"fmt"
	"os"
	"regexp"
	"strings"
	"sync"
	"time"

	"golang.org/x/sync/errgroup"

	"kooix-hajimi/internal/config"
	"kooix-hajimi/internal/github"
	"kooix-hajimi/internal/ratelimit"
	"kooix-hajimi/internal/storage"
	"kooix-hajimi/internal/validator"
	"kooix-hajimi/pkg/logger"
)

// Scanner æ‰«æå™¨
type Scanner struct {
	github     *github.Client
	validator  *validator.Validator
	storage    storage.Storage
	config     config.Config
	
	// æ–°å¢ç»„ä»¶
	deduplicator      *URLDeduplicator
	queryManager      *PhasedQueryManager
	securityNotifier  *github.SecurityNotifier
	
	// çŠ¶æ€ç®¡ç†
	isScanning    bool
	scanMu        sync.RWMutex
	stopCh        chan struct{}
	
	// ç»Ÿè®¡ä¿¡æ¯
	stats         *ScanStats
	statsMu       sync.RWMutex
}

// ScanStats æ‰«æç»Ÿè®¡
type ScanStats struct {
	StartTime         time.Time `json:"start_time"`
	TotalQueries      int       `json:"total_queries"`
	ProcessedQueries  int       `json:"processed_queries"`
	TotalFiles        int       `json:"total_files"`
	ProcessedFiles    int       `json:"processed_files"`
	ValidKeys         int       `json:"valid_keys"`
	RateLimitedKeys   int       `json:"rate_limited_keys"`
	ErrorCount        int       `json:"error_count"`
	CurrentQuery      string    `json:"current_query"`
	IsActive          bool      `json:"is_active"`
}

// New åˆ›å»ºæ–°çš„æ‰«æå™¨
func New(cfg *config.Config) (*Scanner, error) {
	// åˆ›å»ºé™æµç®¡ç†å™¨
	rateLimitConfig := ratelimit.Config{
		RequestsPerMinute: cfg.RateLimit.RequestsPerMinute,
		BurstSize:         cfg.RateLimit.BurstSize,
		CooldownDuration:  cfg.RateLimit.CooldownDuration,
		AdaptiveEnabled:   cfg.RateLimit.AdaptiveEnabled,
		SuccessThreshold:  cfg.RateLimit.SuccessThreshold,
		BackoffMultiplier: cfg.RateLimit.BackoffMultiplier,
	}
	rateLimiter := ratelimit.New(rateLimitConfig)

	// åˆ›å»ºGitHubå®¢æˆ·ç«¯
	githubClient := github.New(cfg.GitHub, rateLimiter)

	// åˆ›å»ºéªŒè¯å™¨
	validatorConfig := validator.Config{
		ModelName:           cfg.Scanner.Validator.ModelName,
		TierDetectionModel:  cfg.Scanner.Validator.TierDetectionModel,
		WorkerCount:         cfg.Scanner.Validator.WorkerCount,
		Timeout:             cfg.Scanner.Validator.Timeout,
		EnableTierDetection: cfg.Scanner.Validator.EnableTierDetection,
	}
	
	// è®¾ç½®é»˜è®¤å€¼ï¼ˆå¦‚æœé…ç½®ä¸­æœªè®¾ç½®ï¼‰
	if validatorConfig.ModelName == "" {
		validatorConfig.ModelName = "gemini-2.5-flash"
	}
	if validatorConfig.TierDetectionModel == "" {
		validatorConfig.TierDetectionModel = "gemini-2.5-flash"
	}
	if validatorConfig.WorkerCount == 0 {
		validatorConfig.WorkerCount = 5
	}
	if validatorConfig.Timeout == 0 {
		validatorConfig.Timeout = 30 * time.Second
	}
	keyValidator := validator.New(validatorConfig)

	// åˆ›å»ºå­˜å‚¨
	var store storage.Storage
	var err error
	
	switch cfg.Storage.Type {
	case "sqlite":
		store, err = storage.NewSQLiteStorage(cfg.Storage)
	default:
		return nil, fmt.Errorf("unsupported storage type: %s", cfg.Storage.Type)
	}
	
	if err != nil {
		return nil, fmt.Errorf("failed to create storage: %w", err)
	}

	return &Scanner{
		github:    githubClient,
		validator: keyValidator,
		storage:   store,
		config:    *cfg,
		
		// åˆå§‹åŒ–æ–°ç»„ä»¶
		deduplicator:     NewURLDeduplicator(),
		queryManager:     NewPhasedQueryManager(),
		securityNotifier: github.NewSecurityNotifier(githubClient, cfg.Scanner.SecurityNotifications.Enabled),
		
		stopCh:    make(chan struct{}),
		stats: &ScanStats{
			StartTime: time.Now(),
		},
	}, nil
}

// ScanWithQueries ä½¿ç”¨æŒ‡å®šæŸ¥è¯¢è¿›è¡Œæ‰«æ
func (s *Scanner) ScanWithQueries(ctx context.Context, queries []string) error {
	if !s.startScanning() {
		return fmt.Errorf("scanner is already running")
	}
	defer s.stopScanning()

	logger.Infof("Starting scan with %d queries", len(queries))
	
	s.updateStats(func(stats *ScanStats) {
		stats.TotalQueries = len(queries)
		stats.IsActive = true
	})

	// æ›´æ–°æ‰«æè¿›åº¦
	progress := &storage.ScanProgress{
		IsScanning:       true,
		QueriesProcessed: 0,
		UpdatedAt:        time.Now(),
	}
	s.storage.UpdateScanProgress(ctx, progress)

	for i, query := range queries {
		select {
		case <-ctx.Done():
			logger.Info("Scan cancelled by context")
			return ctx.Err()
		case <-s.stopCh:
			logger.Info("Scan stopped by user")
			return nil
		default:
		}

		logger.Infof("Processing query %d/%d: %s", i+1, len(queries), query)
		
		s.updateStats(func(stats *ScanStats) {
			stats.CurrentQuery = query
			stats.ProcessedQueries = i + 1
		})

		if err := s.processQuery(ctx, query); err != nil {
			logger.Errorf("Failed to process query '%s': %v", query, err)
			s.updateStats(func(stats *ScanStats) {
				stats.ErrorCount++
			})
			continue
		}

		// æ›´æ–°è¿›åº¦
		progress.QueriesProcessed = i + 1
		progress.LastScanTime = time.Now()
		s.storage.UpdateScanProgress(ctx, progress)
	}

	// å®Œæˆæ‰«æ
	progress.IsScanning = false
	progress.LastScanTime = time.Now()
	s.storage.UpdateScanProgress(ctx, progress)

	logger.Info("Scan completed successfully")
	return nil
}

// processQuery å¤„ç†å•ä¸ªæŸ¥è¯¢
func (s *Scanner) processQuery(ctx context.Context, query string) error {
	// æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦å·²å¤„ç†
	processed, err := s.storage.IsQueryProcessed(ctx, query)
	if err != nil {
		return fmt.Errorf("failed to check if query processed: %w", err)
	}
	
	if processed {
		logger.Infof("Query already processed, skipping: %s", query)
		return nil
	}

	// æœç´¢GitHub
	result, err := s.github.SearchCode(ctx, query)
	if err != nil {
		return fmt.Errorf("github search failed: %w", err)
	}

	if len(result.Items) == 0 {
		logger.Infof("No items found for query: %s", query)
		return s.storage.AddProcessedQuery(ctx, query)
	}

	logger.Infof("Found %d items for query: %s", len(result.Items), query)
	
	s.updateStats(func(stats *ScanStats) {
		stats.TotalFiles += len(result.Items)
	})

	// å¤„ç†æœç´¢ç»“æœ
	if err := s.processSearchItems(ctx, result.Items); err != nil {
		return fmt.Errorf("failed to process search items: %w", err)
	}

	// æ ‡è®°æŸ¥è¯¢å·²å¤„ç†
	return s.storage.AddProcessedQuery(ctx, query)
}

// processSearchItems å¤„ç†æœç´¢ç»“æœ
func (s *Scanner) processSearchItems(ctx context.Context, items []github.SearchItem) error {
	// è¿‡æ»¤é¡¹ç›®
	filteredItems := s.filterItems(items)
	
	if len(filteredItems) == 0 {
		logger.Info("All items filtered out")
		return nil
	}

	logger.Infof("Processing %d filtered items", len(filteredItems))

	// åˆ›å»ºé”™è¯¯ç»„è¿›è¡Œå¹¶å‘å¤„ç†
	g, ctx := errgroup.WithContext(ctx)
	g.SetLimit(s.config.Scanner.WorkerCount) // é™åˆ¶å¹¶å‘æ•°

	// å¤„ç†æ¯ä¸ªé¡¹ç›®
	for _, item := range filteredItems {
		item := item // æ•è·å¾ªç¯å˜é‡
		
		g.Go(func() error {
			select {
			case <-ctx.Done():
				return ctx.Err()
			default:
				return s.processSearchItem(ctx, &item)
			}
		})
	}

	return g.Wait()
}

// processSearchItem å¤„ç†å•ä¸ªæœç´¢é¡¹
func (s *Scanner) processSearchItem(ctx context.Context, item *github.SearchItem) error {
	// æ£€æŸ¥SHAæ˜¯å¦å·²æ‰«æ
	scanned, err := s.storage.IsSHAScanned(ctx, item.SHA)
	if err != nil {
		return fmt.Errorf("failed to check SHA: %w", err)
	}
	
	if scanned {
		logger.Debugf("SHA already scanned, skipping: %s", item.SHA)
		return nil
	}

	// è·å–æ–‡ä»¶å†…å®¹
	content, err := s.github.GetFileContent(ctx, item)
	if err != nil {
		logger.Warnf("Failed to get file content for %s: %v", item.HTMLURL, err)
		return nil // ä¸è¿”å›é”™è¯¯ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
	}

	// æå–APIå¯†é’¥
	keys := s.extractKeys(string(content))
	if len(keys) == 0 {
		// è®°å½•å·²æ‰«æä½†æ— å¯†é’¥çš„æ–‡ä»¶
		s.storage.AddScannedSHA(ctx, item.SHA)
		s.updateStats(func(stats *ScanStats) {
			stats.ProcessedFiles++
		})
		return nil
	}

	logger.Infof("Found %d potential keys in %s", len(keys), item.Path)

	// éªŒè¯å¯†é’¥
	var validKeys []*storage.ValidKey
	var rateLimitedKeys []*storage.RateLimitedKey
	
	for _, keyInfo := range keys {
		results, err := s.validator.ValidateBatch(ctx, []validator.KeyInfo{keyInfo})
		if err != nil {
			logger.Errorf("Validation failed for %s key %s: %v", keyInfo.Provider, keyInfo.Key[:10]+"...", err)
			continue
		}
		
		// å¤„ç†éªŒè¯ç»“æœ
		for _, result := range results {
			switch result.Status {
			case "valid":
				validKeys = append(validKeys, &storage.ValidKey{
					Key:         result.Key,
					Provider:    result.Provider,
					KeyType:     result.Type,
					Source:      "github",
					RepoName:    item.Repository.FullName,
					FilePath:    item.Path,
					FileURL:     item.HTMLURL,
					SHA:         item.SHA,
					ValidatedAt: result.Timestamp,
				})
			case "rate_limited", "quota_exceeded":
				rateLimitedKeys = append(rateLimitedKeys, &storage.RateLimitedKey{
					Key:      result.Key,
					Provider: result.Provider,
					KeyType:  result.Type,
					Source:   "github",
					RepoName: item.Repository.FullName,
					FilePath: item.Path,
					FileURL:  item.HTMLURL,
					SHA:      item.SHA,
					Reason:   result.Status,
				})
			}
		}
	}

	// ä¿å­˜ç»“æœ
	if len(validKeys) > 0 {
		if err := s.storage.SaveValidKeys(ctx, validKeys); err != nil {
			logger.Errorf("Failed to save valid keys: %v", err)
		} else {
			logger.Infof("Saved %d valid keys from %s", len(validKeys), item.Path)
			s.updateStats(func(stats *ScanStats) {
				stats.ValidKeys += len(validKeys)
			})
			
			// å‘é€å®‰å…¨é€šçŸ¥
			s.sendSecurityNotifications(ctx, validKeys, *item)
		}
	}

	if len(rateLimitedKeys) > 0 {
		if err := s.storage.SaveRateLimitedKeys(ctx, rateLimitedKeys); err != nil {
			logger.Errorf("Failed to save rate limited keys: %v", err)
		} else {
			logger.Infof("Saved %d rate limited keys from %s", len(rateLimitedKeys), item.Path)
			s.updateStats(func(stats *ScanStats) {
				stats.RateLimitedKeys += len(rateLimitedKeys)
			})
		}
	}

	// è®°å½•å·²æ‰«æçš„SHA
	s.storage.AddScannedSHA(ctx, item.SHA)
	s.updateStats(func(stats *ScanStats) {
		stats.ProcessedFiles++
	})

	return nil
}

// filterItems è¿‡æ»¤æœç´¢é¡¹
func (s *Scanner) filterItems(items []github.SearchItem) []github.SearchItem {
	var filtered []github.SearchItem

	for _, item := range items {
		// æ£€æŸ¥æ–‡ä»¶è·¯å¾„é»‘åå•
		path := strings.ToLower(item.Path)
		skip := false
		
		for _, blacklisted := range s.config.Scanner.FileBlacklist {
			if strings.Contains(path, strings.ToLower(blacklisted)) {
				skip = true
				break
			}
		}
		
		if skip {
			continue
		}

		// æ£€æŸ¥ä»“åº“å¹´é¾„
		if s.config.Scanner.DateRangeDays > 0 {
			cutoffDate := time.Now().AddDate(0, 0, -s.config.Scanner.DateRangeDays)
			if pushedAt, err := time.Parse("2006-01-02T15:04:05Z", item.Repository.PushedAt); err == nil {
				if pushedAt.Before(cutoffDate) {
					continue
				}
			}
		}

		filtered = append(filtered, item)
	}

	return filtered
}

// extractKeys ä»å†…å®¹ä¸­æå–APIå¯†é’¥
func (s *Scanner) extractKeys(content string) []validator.KeyInfo {
	var allKeys []validator.KeyInfo
	
	// Gemini APIå¯†é’¥æ­£åˆ™è¡¨è¾¾å¼
	geminiPattern := `AIzaSy[A-Za-z0-9\-_]{33}`
	geminiRegex := regexp.MustCompile(geminiPattern)
	geminiMatches := geminiRegex.FindAllString(content, -1)
	
	// OpenAI APIå¯†é’¥æ­£åˆ™è¡¨è¾¾å¼
	openaiPattern := `sk-[A-Za-z0-9]{48}`
	openaiRegex := regexp.MustCompile(openaiPattern)
	openaiMatches := openaiRegex.FindAllString(content, -1)
	
	// OpenAIé¡¹ç›®å¯†é’¥æ­£åˆ™è¡¨è¾¾å¼ (æ–°æ ¼å¼)
	openaiProjectPattern := `sk-proj-[A-Za-z0-9]{48}`
	openaiProjectRegex := regexp.MustCompile(openaiProjectPattern)
	openaiProjectMatches := openaiProjectRegex.FindAllString(content, -1)
	
	// Claude APIå¯†é’¥æ­£åˆ™è¡¨è¾¾å¼
	claudePattern := `sk-ant-api03-[A-Za-z0-9\-_]{95}AA`
	claudeRegex := regexp.MustCompile(claudePattern)
	claudeMatches := claudeRegex.FindAllString(content, -1)
	
	// å¤„ç†Geminiå¯†é’¥
	for _, match := range geminiMatches {
		if !s.isPlaceholderKey(match, content) {
			allKeys = append(allKeys, validator.KeyInfo{
				Key:      match,
				Provider: "gemini",
				Type:     "api_key",
			})
		}
	}
	
	// å¤„ç†OpenAIå¯†é’¥
	for _, match := range openaiMatches {
		if !s.isPlaceholderKey(match, content) {
			allKeys = append(allKeys, validator.KeyInfo{
				Key:      match,
				Provider: "openai",
				Type:     "api_key",
			})
		}
	}
	
	// å¤„ç†OpenAIé¡¹ç›®å¯†é’¥
	for _, match := range openaiProjectMatches {
		if !s.isPlaceholderKey(match, content) {
			allKeys = append(allKeys, validator.KeyInfo{
				Key:      match,
				Provider: "openai",
				Type:     "project_key",
			})
		}
	}
	
	// å¤„ç†Claudeå¯†é’¥
	for _, match := range claudeMatches {
		if !s.isPlaceholderKey(match, content) {
			allKeys = append(allKeys, validator.KeyInfo{
				Key:      match,
				Provider: "claude",
				Type:     "api_key",
			})
		}
	}
	
	// å»é‡
	return s.deduplicateKeys(allKeys)
}

// deduplicateKeys å»é‡å¯†é’¥
func (s *Scanner) deduplicateKeys(keys []validator.KeyInfo) []validator.KeyInfo {
	keySet := make(map[string]bool)
	var result []validator.KeyInfo
	
	for _, keyInfo := range keys {
		if !keySet[keyInfo.Key] {
			keySet[keyInfo.Key] = true
			result = append(result, keyInfo)
		}
	}
	
	return result
}

// isPlaceholderKey æ£€æŸ¥æ˜¯å¦ä¸ºå ä½ç¬¦å¯†é’¥
func (s *Scanner) isPlaceholderKey(key, content string) bool {
	keyIndex := strings.Index(content, key)
	if keyIndex == -1 {
		return false
	}
	
	// æ£€æŸ¥å¯†é’¥å‘¨å›´çš„ä¸Šä¸‹æ–‡
	start := keyIndex - 50
	if start < 0 {
		start = 0
	}
	
	end := keyIndex + len(key) + 50
	if end > len(content) {
		end = len(content)
	}
	
	context := strings.ToUpper(content[start:end])
	
	// æ£€æŸ¥å ä½ç¬¦å…³é”®è¯
	placeholderKeywords := []string{
		"YOUR_", "EXAMPLE", "PLACEHOLDER", "REPLACE", "...", 
		"TODO", "FIXME", "XXX", "SAMPLE",
	}
	
	for _, keyword := range placeholderKeywords {
		if strings.Contains(context, keyword) {
			return true
		}
	}
	
	return false
}

// StartContinuousScanning å¼€å§‹æŒç»­æ‰«æ
func (s *Scanner) StartContinuousScanning(ctx context.Context) error {
	if !s.startScanning() {
		return fmt.Errorf("scanner is already running")
	}
	defer s.stopScanning()

	// åŠ è½½åˆ†é˜¶æ®µæŸ¥è¯¢
	if err := s.queryManager.LoadPhasedQueries(s.config.Scanner.QueryFile); err != nil {
		logger.Errorf("Failed to load phased queries: %v", err)
		// å›é€€åˆ°ä¼ ç»ŸæŸ¥è¯¢åŠ è½½
		return s.startTraditionalScanning(ctx)
	}

	logger.Infof("Starting phased scanning with %d phases", len(s.queryManager.GetPhases()))
	return s.startPhasedScanning(ctx)
}

// startPhasedScanning å¼€å§‹åˆ†é˜¶æ®µæ‰«æ
func (s *Scanner) startPhasedScanning(ctx context.Context) error {
	phases := s.queryManager.GetPhases()
	
	for _, phase := range phases {
		logger.Infof("Starting %s: %s (%d queries)", phase.Name, phase.Description, len(phase.Queries))
		
		for _, query := range phase.Queries {
			select {
			case <-ctx.Done():
				return ctx.Err()
			case <-s.stopCh:
				return nil
			default:
			}
			
			if err := s.scanWithDeduplication(ctx, query, phase.Priority); err != nil {
				logger.Errorf("Error in phase %s query '%s': %v", phase.Name, query, err)
				continue
			}
		}
		
		logger.Infof("Completed %s", phase.Name)
	}
	
	return nil
}

// scanWithDeduplication å¸¦å»é‡çš„æ‰«æ
func (s *Scanner) scanWithDeduplication(ctx context.Context, query string, priority int) error {
	// æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦å·²å¤„ç†
	processed, err := s.storage.IsQueryProcessed(ctx, query)
	if err != nil {
		return fmt.Errorf("failed to check query status: %w", err)
	}
	
	if processed {
		logger.Infof("Query already processed, skipping: %s", query)
		return nil
	}

	// æœç´¢GitHub
	result, err := s.github.SearchCode(ctx, query)
	if err != nil {
		return fmt.Errorf("github search failed: %w", err)
	}

	if len(result.Items) == 0 {
		logger.Infof("No items found for query: %s", query)
		return s.storage.AddProcessedQuery(ctx, query)
	}

	logger.Infof("Found %d items for query: %s", len(result.Items), query)
	
	// æ™ºèƒ½å»é‡
	uniqueItems := s.deduplicateItems(result.Items, priority)
	
	logger.Infof("After deduplication: %d unique items (filtered %d duplicates)", 
		len(uniqueItems), len(result.Items)-len(uniqueItems))
	
	s.updateStats(func(stats *ScanStats) {
		stats.TotalFiles += len(result.Items)
		stats.ProcessedFiles += len(uniqueItems)
	})

	// å¤„ç†å»é‡åçš„ç»“æœ
	if err := s.processSearchItems(ctx, uniqueItems); err != nil {
		return fmt.Errorf("failed to process search items: %w", err)
	}

	// æ ‡è®°æŸ¥è¯¢å·²å¤„ç†
	return s.storage.AddProcessedQuery(ctx, query)
}

// deduplicateItems å¯¹æœç´¢ç»“æœè¿›è¡Œå»é‡
func (s *Scanner) deduplicateItems(items []github.SearchItem, priority int) []github.SearchItem {
	var uniqueItems []github.SearchItem
	
	for _, item := range items {
		if s.deduplicator.AddURL(item.HTMLURL, item.Repository.FullName, item.Path, priority) {
			uniqueItems = append(uniqueItems, item)
		}
	}
	
	return uniqueItems
}

// startTraditionalScanning ä¼ ç»Ÿæ‰«ææ–¹å¼ï¼ˆå‘åå…¼å®¹ï¼‰
func (s *Scanner) startTraditionalScanning(ctx context.Context) error {
	queries, err := s.loadQueries()
	if err != nil {
		return fmt.Errorf("failed to load queries: %w", err)
	}

	logger.Infof("Starting continuous scanning with %d queries", len(queries))

	for {
		select {
		case <-ctx.Done():
			logger.Info("Continuous scanning cancelled")
			return ctx.Err()
		case <-s.stopCh:
			logger.Info("Continuous scanning stopped")
			return nil
		default:
		}

		// æ‰§è¡Œä¸€è½®æ‰«æ
		if err := s.ScanWithQueries(ctx, queries); err != nil {
			logger.Errorf("Scan round failed: %v", err)
		}

		// ç­‰å¾…ä¸‹ä¸€è½®
		logger.Infof("Waiting %v before next scan round", s.config.Scanner.ScanInterval)
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-s.stopCh:
			return nil
		case <-time.After(s.config.Scanner.ScanInterval):
		}
	}
}

// loadQueries åŠ è½½æŸ¥è¯¢åˆ—è¡¨
func (s *Scanner) loadQueries() ([]string, error) {
	file, err := os.Open(s.config.Scanner.QueryFile)
	if err != nil {
		return nil, fmt.Errorf("failed to open query file: %w", err)
	}
	defer file.Close()

	var queries []string
	scanner := bufio.NewScanner(file)
	
	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())
		if line == "" || strings.HasPrefix(line, "#") {
			continue
		}
		queries = append(queries, line)
	}

	if err := scanner.Err(); err != nil {
		return nil, fmt.Errorf("failed to read query file: %w", err)
	}

	return queries, nil
}

// Stop åœæ­¢æ‰«æ
func (s *Scanner) Stop() {
	close(s.stopCh)
}

// GetStats è·å–æ‰«æç»Ÿè®¡
func (s *Scanner) GetStats() *ScanStats {
	s.statsMu.RLock()
	defer s.statsMu.RUnlock()
	
	// è¿”å›å‰¯æœ¬
	stats := *s.stats
	return &stats
}

// IsScanning æ£€æŸ¥æ˜¯å¦æ­£åœ¨æ‰«æ
func (s *Scanner) IsScanning() bool {
	s.scanMu.RLock()
	defer s.scanMu.RUnlock()
	return s.isScanning
}

// startScanning å¼€å§‹æ‰«æ
func (s *Scanner) startScanning() bool {
	s.scanMu.Lock()
	defer s.scanMu.Unlock()
	
	if s.isScanning {
		return false
	}
	
	s.isScanning = true
	s.updateStats(func(stats *ScanStats) {
		stats.IsActive = true
		stats.StartTime = time.Now()
	})
	
	return true
}

// stopScanning åœæ­¢æ‰«æ
func (s *Scanner) stopScanning() {
	s.scanMu.Lock()
	defer s.scanMu.Unlock()
	
	s.isScanning = false
	s.updateStats(func(stats *ScanStats) {
		stats.IsActive = false
	})
}

// updateStats æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
func (s *Scanner) updateStats(fn func(*ScanStats)) {
	s.statsMu.Lock()
	defer s.statsMu.Unlock()
	fn(s.stats)
}

// GetTokenStates è·å–GitHub tokençŠ¶æ€ä¿¡æ¯
func (s *Scanner) GetTokenStates() map[string]interface{} {
	tokenInfo := s.github.GetRateLimitInfo()
	result := make(map[string]interface{})
	
	for token, state := range tokenInfo {
		// åªè¿”å›éæ•æ„Ÿä¿¡æ¯
		maskedToken := token[:8] + "***" + token[len(token)-4:]
		result[maskedToken] = map[string]interface{}{
			"remaining":     state.Remaining,
			"reset_time":    state.ResetTime,
			"cooldown_end":  state.CooldownEnd,
			"last_used":     state.LastUsed,
			"success_rate":  state.SuccessRate,
			"request_count": state.RequestCount,
		}
	}
	
	return result
}

// sendSecurityNotifications å‘é€å®‰å…¨é€šçŸ¥
func (s *Scanner) sendSecurityNotifications(ctx context.Context, validKeys []*storage.ValidKey, item github.SearchItem) {
	if !s.config.Scanner.SecurityNotifications.Enabled {
		return
	}

	for _, validKey := range validKeys {
		// ç¡®å®šä¸¥é‡çº§åˆ«
		severity := s.determineSeverityByProvider(validKey.Provider)
		
		// æ£€æŸ¥æ˜¯å¦éœ€è¦é€šçŸ¥
		if !s.shouldNotify(severity) {
			continue
		}

		// åˆ›å»ºæ³„éœ²å¯†é’¥ä¿¡æ¯
		leakedInfo := github.LeakedKeyInfo{
			KeyType:     validKey.KeyType,
			Provider:    validKey.Provider,
			Repository:  item.Repository.FullName,
			FilePath:    item.Path,
			URL:         item.HTMLURL,
			KeyPreview:  validKey.Key[:10],
			DiscoveredAt: time.Now(),
			Severity:    severity,
		}

		// è®°å½•å®‰å…¨äº‹ä»¶
		logger.Warnf("ğŸš¨ SECURITY ALERT: %s API key found in %s/%s", 
			validKey.Provider, item.Repository.FullName, item.Path)

		// åˆ›å»ºGitHub issueï¼ˆå¦‚æœå¯ç”¨ï¼‰
		if s.config.Scanner.SecurityNotifications.CreateIssues {
			if s.config.Scanner.SecurityNotifications.DryRun {
				logger.Infof("DRY RUN: Would create security issue for %s in %s", 
					validKey.Provider, item.Repository.FullName)
			} else {
				if err := s.securityNotifier.CreateSecurityIssue(ctx, leakedInfo); err != nil {
					logger.Errorf("Failed to create security issue: %v", err)
				} else {
					logger.Infof("âœ… Created security issue for %s key in %s", 
						validKey.Provider, item.Repository.FullName)
				}
			}
		}
	}
}

// determineSeverityByProvider æ ¹æ®Providerç¡®å®šå¯†é’¥æ³„éœ²çš„ä¸¥é‡çº§åˆ«
func (s *Scanner) determineSeverityByProvider(provider string) string {
	switch provider {
	case "openai":
		return "high"
	case "gemini", "google":
		return "high"
	case "claude", "anthropic":
		return "high"
	case "aws":
		return "critical" // AWS keyæ³„éœ²é£é™©æé«˜
	case "github":
		return "critical" // GitHub PATé£é™©å¾ˆé«˜
	case "gitlab":
		return "high"
	case "stripe":
		return "critical" // æ”¯ä»˜ç›¸å…³
	default:
		return "medium"
	}
}

// shouldNotify åˆ¤æ–­æ˜¯å¦åº”è¯¥å‘é€é€šçŸ¥
func (s *Scanner) shouldNotify(severity string) bool {
	switch s.config.Scanner.SecurityNotifications.NotifyOnSeverity {
	case "all":
		return true
	case "critical":
		return severity == "critical"
	case "high":
		return severity == "critical" || severity == "high"
	default:
		return severity == "critical" || severity == "high"
	}
}